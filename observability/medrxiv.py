"""The preprint server for health sciences."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_medrxiv.ipynb.

# %% auto 0
__all__ = ['parent_dir', 'log_dir', 'data_dir', 'url', 'response', 'soup', 'items', 'data', 'filepath', 'new_data',
           'deduplicated']

# %% ../nbs/01_medrxiv.ipynb 3
import warnings
warnings.filterwarnings("ignore")

import os
import logging

import pandas as pd
import requests
from bs4 import BeautifulSoup

# %% ../nbs/01_medrxiv.ipynb 4
try:
    # This will work when running as a script
    script_dir = os.path.dirname(os.path.abspath(__file__))
except NameError:
    # This will work when running in a Jupyter notebook
    script_dir = os.getcwd()

parent_dir = os.path.abspath(os.path.join(script_dir, os.pardir))
log_dir = os.path.join(parent_dir, 'logs')
data_dir = os.path.join(parent_dir, 'data')

if not os.path.exists(log_dir):
    os.makedirs(log_dir)

if not os.path.exists(data_dir):
    os.makedirs(data_dir)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', filename=os.path.join(log_dir, 'medrxiv.log'), filemode='a')

# %% ../nbs/01_medrxiv.ipynb 5
url = "https://connect.medrxiv.org/medrxiv_xml.php?subject=all"
response = requests.get(url)
response.raise_for_status()

# %% ../nbs/01_medrxiv.ipynb 6
soup = BeautifulSoup(response.content, "lxml-xml")
items = soup.find_all("item")

# %% ../nbs/01_medrxiv.ipynb 7
data = []
for item in items:
    item_data = {}
    for child in item.find_all(recursive=False):
        tag_name = child.name
        tag_value = child.text.strip() if child.text else None
        item_data[tag_name] = tag_value
    item_data.update(item.attrs)
    data.append(item_data)
data[0]

# %% ../nbs/01_medrxiv.ipynb 8
filepath = os.path.join(data_dir, "medrxiv.jsonl")

new_data = pd.DataFrame(data)
if os.path.exists(filepath):
    existing_data = pd.read_json(filepath, lines=True)
    combined_data = pd.concat([existing_data, new_data])
else:
    combined_data = new_data

deduplicated = combined_data.drop_duplicates(subset="identifier")
deduplicated.to_json(filepath, orient="records", lines=True)
logging.info('Total number of records: {}'.format(deduplicated.shape[0]))
